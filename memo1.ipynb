{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #### RAG system so that it can answer questions both from the uploaded PDF and from external knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from groq import Groq  # Correct import for Groq\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the document\n",
    "file_path = r\"wp-oreilly-deep-learning.pdf\"\n",
    "if file_path.endswith(\".pdf\"):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "elif file_path.endswith(\".docx\"):\n",
    "    loader = Docx2txtLoader(file_path)\n",
    "else:\n",
    "    raise ValueError(\"File format not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wp-oreilly-deep-learning.pdf'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.endswith(\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.from_documents(all_splits, embeddings)\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializes ConversationBufferMemory to store the conversation history.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text describes how logistic regression works. In logistic regression, a logistic function is used to determine the probability of an outcome based on input values and weights. The logistic function, also known as the sigmoid function, maps any input in the range of negative to positive infinity to an output in the range of 0.0 to 1.0, allowing the output to be interpreted as a probability.\n",
      "\n",
      "The process involves the following steps:\n",
      "\n",
      "1. **Defining the logistic function**: The logistic function is defined as f(x) = 1 / (1 + e^(-θx)), where θ is the weight and x is the input value.\n",
      "2. **Calculating the output**: The output of the logistic function is calculated by plugging in the input values and weights into the function.\n",
      "3. **Interpreting the output**: The output of the logistic function is interpreted as a probability, with values close to 1 indicating a high probability of the outcome and values close to 0 indicating a low probability.\n",
      "4. **Training the model**: The model is trained using a dataset, where the goal is to find the optimal weights that minimize the error between the predicted probabilities and the actual outcomes.\n",
      "5. **Making predictions**: Once the model is trained, it can be used to make predictions on new, unseen data by plugging in the input values and using the learned weights to calculate the predicted probability.\n",
      "\n",
      "The text also mentions that the logistic function is often used in neural networks, where it is used as an activation function to introduce non-linearity into the model. The activation function is used to determine the output of each node in the network, and the logistic function is a common choice for this purpose.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "# model = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Chat History:\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain with hybrid retrieval\n",
    "def hybrid_qa_chain(query, retriever, memory):\n",
    "    # Retrieve from PDF first\n",
    "    pdf_results = retriever.get_relevant_documents(query)\n",
    "    if pdf_results:\n",
    "        context = \"\\n\".join([doc.page_content for doc in pdf_results])\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model= \"llama-3.3-70b-versatile\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt.format(context=context, question=query, history=memory.buffer, input=query)},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: It looks like you're saying hello again. Hello! Is there something I can help you with or would you like to chat? By the way, I think I recall you mentioning your name might be Sid earlier. Is that correct?\n",
      "AI: Hello Sid from Merta. It's nice to meet you. Is there something I can help you with or would you like to chat? By the way, I see you've already mentioned your name and location a few times in our conversation. What brings you here today?\n",
      "AI: Your name is Sid, and you are from Merta.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Example conversation loop\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "    # Generate a response using the hybrid QA chain\n",
    "    response = hybrid_qa_chain(user_query, vector_store.as_retriever(), memory)\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "    # Save the conversation to memory\n",
    "    memory.save_context({\"input\": user_query}, {\"output\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
